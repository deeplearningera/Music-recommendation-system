{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cnn",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JADQMwh2UCbX"
      },
      "outputs": [],
      "source": [
        "from keras.models import load_model\n",
        "import numpy as np\n",
        "import cv2\n",
        "from keras.preprocessing import image\n",
        "\n",
        "detection_model_path = 'haarcascade_files/haarcascade_frontalface_default.xml'\n",
        "emotion_model_path = 'final_model.h5'\n",
        "face_detection = cv2.CascadeClassifier(detection_model_path)\n",
        "emotion_classifier = load_model(emotion_model_path, compile=False)\n",
        "EMOTIONS = [\"happy\", \"sad\"]\n",
        "\n",
        "class cnn():\n",
        "    def emotion_testing():\n",
        "        cap = cv2.VideoCapture(0)\n",
        "        while True:\n",
        "            ret, test_img = cap.read()\n",
        "            if not ret:\n",
        "                continue\n",
        "            gray_img = cv2.cvtColor(test_img, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "            faces_detected = face_detection.detectMultiScale(gray_img, 1.32, 5)\n",
        "\n",
        "            for (x, y, w, h) in faces_detected:\n",
        "                cv2.rectangle(test_img, (x, y), (x + w, y + h), (255, 0, 0), thickness=7)\n",
        "                roi_gray = gray_img[y:y + w, x:x + h]  # cropping region of interest i.e. face area from  image\n",
        "                roi_gray = cv2.resize(roi_gray, (48, 48))\n",
        "                img_pixels = image.img_to_array(roi_gray)\n",
        "                img_pixels = np.expand_dims(img_pixels, axis=0)\n",
        "                img_pixels /= 255\n",
        "\n",
        "                predictions = emotion_classifier.predict(img_pixels)\n",
        "\n",
        "                # find max indexed array\n",
        "                max_index = np.argmax(predictions[0])\n",
        "                predicted_emotion = EMOTIONS[max_index]\n",
        "\n",
        "                cv2.putText(test_img, predicted_emotion, (int(x), int(y)), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
        "\n",
        "            resized_img = cv2.resize(test_img, (1000, 700))\n",
        "            cv2.imshow('Facial emotion analysis ', resized_img)\n",
        "\n",
        "            if cv2.waitKey(0) & 0xFF == ord('q'):\n",
        "                break\n",
        "        cap.release()\n",
        "        cv2.destroyAllWindows\n",
        "        return predicted_emotion"
      ]
    }
  ]
}